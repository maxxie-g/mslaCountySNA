{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('stopwords')\n",
    "import nltk.corpus\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Plotting\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Utilites\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "# APIs\n",
    "import pymongo\n",
    "import tweepy\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo = \"mongodb://localhost:27017/\"\n",
    "twitter = \"AAAAAAAAAAAAAAAAAAAAAEy0dgEAAAAABmdX43JXQabHK7JN9DytAkFbQXs%3DVopruHesemfSWbQa3WMU1hORYsikSYFFBUUzLY4ul1heniWCum\"\n",
    "\n",
    "dbclient = pymongo.MongoClient(mongo) # Initiate connection to MongoDB\n",
    "api = tweepy.Client(bearer_token=twitter, wait_on_rate_limit=True, return_type=[dict]) # Initiate tweepy client\n",
    "\n",
    "pFollower = dbclient.sna_database.primaryFollowers\n",
    "sFollower = dbclient.sna_database.secondaryFollowers\n",
    "sFollowing = dbclient.sna_database.secondaryFollowing\n",
    "combined = dbclient.sna_database.combined\n",
    "sna_database = dbclient.sna_database\n",
    "df_pFollower = pd.DataFrame(list(pFollower.find()))\n",
    "df_sFollower = pd.DataFrame(list(sFollower.find().sort(\"_id\",1)))\n",
    "df_sFollowing = pd.DataFrame(list(sFollowing.find().sort(\"_id\",1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for cleaning up text, making it easier for NLP\n",
    "def nlp_preprocessing(group,dataset,stemOutputPath,lemmaOutputPath):\n",
    "    # get the text from group where the description is not empty\n",
    "    \n",
    "    list_temp = list(dataset['description'])\n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    for i in range(len(list_temp)):\n",
    "        # normalize text\n",
    "        list_temp[i] = list_temp[i].lower()\n",
    "            \n",
    "        # remove unicode characters, mentions, and retweets\n",
    "        list_temp[i] = re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\" + \"\\\\n\", \" \", list_temp[i])\n",
    "\n",
    "        # remove stopwords\n",
    "        list_temp[i] = \" \".join([word for word in list_temp[i].split() if word not in (stop)])\n",
    "    \n",
    "    # we need to iterate back through the list and remove any empty strings\n",
    "    text = [item for item in list_temp if item != \"\" if item != \"n/a\"]\n",
    "    \n",
    "    # stemming\n",
    "    text_stem = text\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    for s in range(len(text_stem)):\n",
    "        text_stem[s] = stemmer.stem(text_stem[s])\n",
    "\n",
    "    # lemmatization\n",
    "    text_lemma = text\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for l in range(len(text_lemma)):\n",
    "        text_lemma[l] = lemmatizer.lemmatize(text_lemma[l])\n",
    "    \n",
    "    # output the stemmed and lemmatized texts to csv for storage\n",
    "    stem_series = pd.Series(text_stem)\n",
    "    stem_series.to_csv(stemOutputPath,index=False)\n",
    "\n",
    "    lemma_series = pd.Series(text_lemma)\n",
    "    lemma_series.to_csv(lemmaOutputPath,index=False)\n",
    "\n",
    "    return print(\"Complete Group \" + str(group))\n",
    "\n",
    "# Method for creating the wordcloud from the results of the nlp_preprocessing()\n",
    "def wordcloud_vis(lemmaInput,stemInput,lemmaOutput,stemOutput):\n",
    "    stop = stopwords.words('english')\n",
    "    stop.extend(['missoula','montana','montanan','mt'])\n",
    "\n",
    "    text_lemma = \" \".join(lemmaInput)\n",
    "    wordcloud = WordCloud(stopwords=stop,width=1600,height=1600).generate(text_lemma)\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    wordcloud.to_file(lemmaOutput)\n",
    "\n",
    "    text_stem = \" \".join(stemInput)\n",
    "    wordcloud = WordCloud(stopwords=stop,width=1600,height=1600).generate(text_stem)\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    wordcloud.to_file(stemOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secondary Followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP of descriptions filtered by group\n",
    "\n",
    "for i in range(6):\n",
    "    print(\"Preprocessing\")\n",
    "    stemToPath = \"sFollowers/descriptions/stemmed/group%s.csv\" % i\n",
    "    lemmaToPath = \"sFollowers/descriptions/lemmatized/group%s.csv\" % i\n",
    "    query = {'group': i, 'description':{'$ne':''}}\n",
    "    temp = pd.DataFrame(list(sFollower.find(query)))\n",
    "    nlp_preprocessing(i,temp,stemToPath,lemmaToPath)\n",
    "\n",
    "    print(\"wordcloud Generation\")\n",
    "    lemmaWordcloudOutput = \"dataVis/sFollowers/wordcloud/lemmatized/group%s.png\" % i\n",
    "    temp_lemma = pd.read_csv(\"sFollowers/descriptions/lemmatized/group%s.csv\" % i)\n",
    "    lemma = list(temp_lemma['0'])\n",
    "\n",
    "    stemWordcloudOutput = \"dataVis/sFollowers/wordcloud/stemmed/group%s.png\" % i\n",
    "    temp_stem = pd.read_csv(\"sFollowers/descriptions/stemmed/group%s.csv\" % i)\n",
    "    stem = list(temp_stem['0'])\n",
    "    wordcloud_vis(lemma,stem,lemmaWordcloudOutput,stemWordcloudOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis of the Influencers\n",
    "First, we will analyze the influencer of the combined dataset: ryanpcooney. In order to analyze the tweets, we must first procure the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing ryanpcooney\n",
    "tweets_text = []\n",
    "tweets_text_list = []\n",
    "tweets_id = []\n",
    "tweets_id_list = []\n",
    "\n",
    "for response in tweepy.Paginator(api.get_users_tweets, id=50103302, max_results = 100, limit=100, exclude=\"retweets\"):\n",
    "    print(response)\n",
    "    for tweet in response.data:\n",
    "        print(tweet)\n",
    "        tweets_id.append(tweet.id)\n",
    "        tweets_text.append(tweet.text)\n",
    "\n",
    "tweets_id_list.append(tweets_id)\n",
    "tweets_text_list.append(tweets_text)\n",
    "df_tweets = pd.DataFrame(columns=[\"id\",\"text\"])\n",
    "df_tweets[\"text\"] = tweets_text_list[0]\n",
    "df_tweets[\"id\"] = tweets_id_list[0]\n",
    "\n",
    "df_tweets.to_csv(\"influencers/combined/ryanpcooney/tweets.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and remove emojis\n",
    "RE_EMOJI = re.compile(\n",
    "    \"[\"\n",
    "    \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "    \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "    \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "    \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "    \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "    \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "    \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "    \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "    \"\\U000024C2-\\U0001F251\"\n",
    "    \"]+\"\n",
    "    )\n",
    "\n",
    "def cleaning_emojis(text):\n",
    "    return RE_EMOJI.sub(r'',text)\n",
    "df_tweets[\"text\"] = df_tweets[\"text\"].apply(lambda x: cleaning_emojis(x))\n",
    "df_tweets[\"text\"].to_csv(\"test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and remove line breaks\n",
    "def cleaning_linebreaks(text):\n",
    "    return re.sub(r'\\n',' ',text)\n",
    "df_tweets[\"text\"] = df_tweets[\"text\"].apply(lambda x: cleaning_linebreaks(x))\n",
    "df_tweets[\"text\"].to_csv(\"test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning and removing retweets\n",
    "def cleaning_retweets(text):\n",
    "    return re.sub(r'(RT.+)',' ',text)\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: cleaning_retweets(x))\n",
    "tweets.drop_duplicates(subset=\"text\",inplace=True)\n",
    "tweets[\"text\"].to_csv(\"test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning and removing mentions\n",
    "def cleaning_mentions(text):\n",
    "    return re.sub(r'(@[A-Za-z0-9_]+)',' ',text)\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: cleaning_mentions(x))\n",
    "tweets[\"text\"].to_csv(\"test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize capitalization of the tweets\n",
    "df_tweets[\"text\"] = df_tweets[\"text\"].str.lower()\n",
    "df_tweets[\"text\"].to_csv(\"test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and remove URLs\n",
    "def cleaning_URLs(text):\n",
    "    return re.sub('(www.[^s]+)|(https?://[^s]+)|(`)',' ',text)\n",
    "df_tweets[\"text\"] = df_tweets[\"text\"].apply(lambda x: cleaning_URLs(x))\n",
    "df_tweets[\"text\"].to_csv(\"test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning and removing punctuations\n",
    "english_punctuation = string.punctuation\n",
    "punctuations_list = english_punctuation\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('','',english_punctuation)\n",
    "    return text.translate(translator)\n",
    "df_tweets[\"text\"] = df_tweets[\"text\"].apply(lambda x: cleaning_punctuations(x))\n",
    "df_tweets[\"text\"] = df_tweets[\"text\"].apply(lambda x: re.sub(r'(’)|(”)|(“)|(\")','',x))\n",
    "df_tweets[\"text\"].to_csv(\"test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "stop = stopwords.words('english')\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop])\n",
    "df_tweets[\"text\"] = df_tweets[\"text\"].apply(lambda text: cleaning_stopwords(text))\n",
    "df_tweets[\"text\"].to_csv(\"test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.drop_duplicates(subset=\"text\",inplace=True)\n",
    "df_tweets[\"text\"].to_csv(\"test.csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem the words\n",
    "stemmer = PorterStemmer()\n",
    "text_stem = list(df_tweets[\"text\"])\n",
    "\n",
    "for s in range(len(text_stem)):\n",
    "    text_stem[s] = stemmer.stem(text_stem[s])\n",
    "stem_series = pd.Series(text_stem)\n",
    "stem_series.to_csv(\"influencers/combined/ryanpcooney/tweets_stem.csv\",header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize the words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "text_lemma = list(df_tweets[\"text\"])\n",
    "\n",
    "for l in range(len(text_lemma)):\n",
    "    text_lemma[l] = lemmatizer.lemmatize(text_lemma[l])\n",
    "lemma_series = pd.Series(text_lemma)\n",
    "lemma_series.to_csv(\"influencers/combined/ryanpcooney/tweets_lemma.csv\",header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x7f78c14144c0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a wordcloud from the \n",
    "stop = stopwords.words('english')\n",
    "wc = WordCloud(width=1600,height=800).generate(\" \".join(text_lemma))\n",
    "wc.to_file(\"influencers/combined/ryanpcooney/vis/wordclound.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Iterate through all influencers of the modularity classes and create wordclouds from the lemmatized tweets. When outputting all data, handle unmade files (make file if not exist).\n",
    "#TODO: Create wordclouds for either each user, or modularity class.\n",
    "#TODO: Perfect the preprocessing process (Handle unseen characters, line breaks, etc) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('sna')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9031bf188e25d3bc949e5c8a042f665a15ac04e169b8b6b5e0691fc0c6e29585"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
