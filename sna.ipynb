{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Network Analysis of the Missoula County Twitter Account\n",
    "\n",
    "Note: this project is heavily influenced by Steve Hedden's Article __[_\"How to download and visualize your Twitter network\"_](https://towardsdatascience.com/how-to-download-and-visualize-your-twitter-network-f009dbbf107b)__\n",
    "## Initiate and import required dependencies and APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from community import community_louvain\n",
    "\n",
    "# Get API Keys\n",
    "file_path = 'keys.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    keys = file.readlines()\n",
    "\n",
    "# Set up tweepy client\n",
    "bearer_token = keys[1]\n",
    "access_token = keys[2]\n",
    "access_token_secret = keys[3]\n",
    "\n",
    "api = tweepy.Client(bearer_token=bearer_token, wait_on_rate_limit=True, return_type=[dict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary Followers\n",
    "### Data Acquisition\n",
    "Using the Tweepy **Paginator** method we are able to request a user's followers, without needing to redo the call after every 1000 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = [\"3178252764\"]\n",
    "follower_list = []\n",
    "for user in user_id:\n",
    "    followers = []\n",
    "    try:\n",
    "        for response in tweepy.Paginator(api.get_users_followers, 3178252764, max_results=1000):\n",
    "            for follower in response.data:\n",
    "                followers.append(follower.id)\n",
    "    except tweepy.TweepyException:\n",
    "        print(\"error\")\n",
    "        continue\n",
    "    follower_list.append(followers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame of the _primary followers_\n",
    "After the DataFrame has been created, we export it to a .csv for further analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['source','target'])\n",
    "df['source'] = follower_list[0]\n",
    "df['target'] = 3178252764 # User ID of the Missoula County account\n",
    "\n",
    "df.to_csv(\"edges.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"finalVis/pFollowers1.png\" alt-text=\"primary-followers\" title=\"Primary Followers\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secondary Followers\n",
    "We reuse the code from the _primary followers_, but this time we iterate through the '**target**' column of the DataFrame. <br>\n",
    "Once that is complete we check to see if all users have been queried, as it is possible that the code skipped users, or that the loops broke too soon, thus forcing us to restart the code.\n",
    "\n",
    "Here we are already deciding to remove users with 0 followers, as they don't generate any usable connection that we might need for the analysis down the road.\n",
    "If a user returns **`None`**, we skip that user as they have a private account, so we can't obtain any data off of them.\n",
    "\n",
    "Here we are obtaining the user IDs of people that are following the queried user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list = list(df['target'])\n",
    "secondary_df = pd.DataFrame(columns=['source','target'])\n",
    "\n",
    "for userID in user_list:\n",
    "    print(userID)\n",
    "    secondary_followers = []\n",
    "    secondary_follower_list = []\n",
    "\n",
    "    for response in tweepy.Paginator(api.get_users_followers, userID, max_results=1000):\n",
    "        if response.data==None:\n",
    "            break\n",
    "        for follower in response.data:\n",
    "            secondary_followers.append(follower.id)\n",
    "\n",
    "    if len(secondary_followers)>0:\n",
    "        secondary_follower_list.append(secondary_followers)\n",
    "        temp = pd.DataFrame(columns=['source','target'])\n",
    "        temp['target'] = secondary_follower_list[0]\n",
    "        temp['source'] = userID\n",
    "        secondary_df = secondary_df.append(temp)\n",
    "        secondary_df.to_csv(\"sFollowers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify all users have been queried\n",
    "\n",
    "Once we have obtained a completed list of _seconday followers_, we check and verify that all users have been queried and are in the list/DataFrame/file.\n",
    "\n",
    "No outputs = all users were queried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sFollowers_df = pd.read_csv(\"sFollowers.csv\")\n",
    "sFollowers = list(sFollowers_df['source'])\n",
    "\n",
    "for userID in user_list:\n",
    "    if userID in sFollowers:\n",
    "        break\n",
    "    else:\n",
    "        print(str(userID) + \" is not in the list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preliminary Analysis\n",
    "\n",
    "After the data has been obtained, checked, and verified, we need to clean it, as there are too many datapoints for Gephi to handle.\n",
    "\n",
    "Here we use NetworkX for the network analysis portion. Once we have converted the DataFrame into a graph, we run `G.number_of_nodes()`, it returns **1.280.569**, which is far too many to effectively analyze. Now we use `k_core()` to pare down the Graph to nodes with `degree > 10`, which is roughly the top 1% of users. Now when we run `G_tmp.number_of_nodes()`, we get **12.791**, a much more manageable number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sFollowers_df = pd.read_csv(\"sFollowers/sFollowers.csv\")\n",
    "G = nx.from_pandas_edgelist(sFollowers_df, 'source', 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_tmp = nx.k_core(G, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Detection\n",
    "Now that we have a Dataset with which we can work, we now run a community detection algorithm to split the Data into groups. We then turn the partition into a DataFrame, with columns '`names`' and '`group`'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = community_louvain.best_partition(G_tmp)\n",
    "\n",
    "partition1 = pd.DataFrame([partition]).T\n",
    "partition1 = partition1.reset_index()\n",
    "partition1.columns = ['names','group']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree Centrality\n",
    "Now we sort the Dataset using _degree centrality_, allowing us to see the most influential nodes of the Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16249481</td>\n",
       "      <td>7516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69365437</td>\n",
       "      <td>6296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17139587</td>\n",
       "      <td>5961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19188212</td>\n",
       "      <td>4374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21316568</td>\n",
       "      <td>4195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      names  degree\n",
       "0  16249481    7516\n",
       "1  69365437    6296\n",
       "2  17139587    5961\n",
       "3  19188212    4374\n",
       "4  21316568    4195"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_sorted = pd.DataFrame(sorted(G_tmp.degree, key=lambda x: x[1], reverse=True))\n",
    "G_sorted.columns = ['names','degree']\n",
    "dc = G_sorted\n",
    "G_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readability\n",
    "For easier readability of the visualization, we need to obtain the Twitter Handles of the users. For the sake of clarity we only obtain the first **100** user handles, which coincides with the **100** most influential nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list = list(G_sorted[\"names\"])\n",
    "sFollowersHandles_df = pd.DataFrame(columns=['Id','handle'])\n",
    "sFollowersHandles = []\n",
    "sFollowersHandles_list = []\n",
    "user_df = []\n",
    "user_list_df = []\n",
    "\n",
    "for i in range(100):\n",
    "    response = api.get_user(id=user_list[i])\n",
    "    sFollowersHandles.append(response.data.username)\n",
    "    user_df.append(user_list[i])\n",
    "    print(str(user_list[i]) + ' ' + sFollowersHandles[i])\n",
    "    sFollowersHandles_list.append(sFollowersHandles)\n",
    "    user_list_df.append(user_df)\n",
    "\n",
    "sFollowersHandles_df['handle'] = sFollowersHandles_list[0]\n",
    "sFollowersHandles_df['Id'] = user_list_df[0]\n",
    "sFollowersHandles_df.to_csv(\"sFollowers/sFollowersHandles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export\n",
    "After the preliminary analysis and data cleaning, we know need to export the cleaned data for further analysis using Gephi. To do that we combine the sorted DataFrame with the Groups from the community detection algorithm, giving us a DataFrame of nodes. Then we convert the `G_tmp` NetworkX graph back into a Pandas Edgelist DataFrame. Then we export both DataFrames as `.csv` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.merge(dc, partition1, how='left', left_on='names', right_on='names')\n",
    "combined = combined.rename(columns={'names':'id'})\n",
    "\n",
    "edges = nx.to_pandas_edgelist(G_tmp)\n",
    "nodes = combined['id']\n",
    "\n",
    "edges.to_csv(\"edges.csv\")\n",
    "combined.to_csv(\"nodes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"finalVis/sFollowers.png\" alt=\"data-viz\" title=\"Viz\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secondary Following\n",
    "We reuse the code from the _primary followers_, but this time we iterate through the '**target**' column of the DataFrame. <br>\n",
    "Once that is complete we check to see if all users have been queried, as it is possible that the code skipped users, or that the loops broke too soon, thus forcing us to restart the code.\n",
    "\n",
    "Here we are already deciding to remove users with 0 followers, as they don't generate any usable connection that we might need for the analysis down the road.\n",
    "If a user returns **`None`**, we skip that user as they have a private account, so we can't obtain any data off of them.\n",
    "\n",
    "This is the exact same procedure as with the _secondary followers_, but this time we are obtaining the user IDs of who the queried user is following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list = list(df['target'])\n",
    "secondary_df = pd.DataFrame(columns=['source','target'])\n",
    "\n",
    "for userID in user_list:\n",
    "    print(userID)\n",
    "    secondary_following = []\n",
    "    secondary_following_list = []\n",
    "\n",
    "    for response in tweepy.Paginator(api.get_users_following, userID, max_results=1000):\n",
    "        if response.data==None:\n",
    "            break\n",
    "        for following in response.data:\n",
    "            secondary_following.append(following.id)\n",
    "\n",
    "    if len(secondary_following)>0:\n",
    "        secondary_following_list.append(secondary_following)\n",
    "        temp = pd.DataFrame(columns=['source','target'])\n",
    "        temp['target'] = secondary_following_list[0]\n",
    "        temp['source'] = userID\n",
    "        secondary_df = secondary_df.append(temp)\n",
    "        secondary_df.to_csv(\"sFollowing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify all users have been queried\n",
    "\n",
    "Once we have obtained a completed list of _seconday following_, we check and verify that all users have been queried and are in the list/DataFrame/file.\n",
    "\n",
    "No outputs = all users were queried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sFollowing_df = pd.read_csv(\"sFollowing/sFollowing.csv\")\n",
    "sFollowing = list(secondary_df['source'])\n",
    "\n",
    "for userID in user_list:\n",
    "    if userID in sFollowing:\n",
    "        break\n",
    "    else:\n",
    "        print(str(userID) + \" is not in the list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preliminary Analysis\n",
    "\n",
    "After the data has been obtained, checked, and verified, we need to clean it, as there are too many datapoints for Gephi to handle.\n",
    "\n",
    "Here we use NetworkX for the network analysis portion. Once we have converted the DataFrame into a graph, we run `G.number_of_nodes()`, it returns **988.551**, which is far too many to effectively analyze. Now we use `k_core()` to pare down the Graph to nodes with `degree>15`. Now when we run `G_tmp.number_of_nodes()`, we get **14.778**, a much more manageable number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sFollowing_df = pd.read_csv(\"sFollowing/sFollowing.csv\")\n",
    "G = nx.from_pandas_edgelist(sFollowing_df, 'source', 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_tmp = nx.k_core(G, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Detection\n",
    "Now that we have a Dataset with which we can work, we now run a community detection algorithm to split the Data into groups. We then turn the partition into a DataFrame, with columns '`names`' and '`group`'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = community_louvain.best_partition(G_tmp)\n",
    "\n",
    "partition1 = pd.DataFrame([partition]).T\n",
    "partition1 = partition1.reset_index()\n",
    "partition1.columns = ['names','group']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree Centrality\n",
    "Now we sort the Dataset using _degree centrality_, allowing us to see the most influential nodes of the Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36167088</td>\n",
       "      <td>3235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17037258</td>\n",
       "      <td>2674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>707475104</td>\n",
       "      <td>2642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22812156</td>\n",
       "      <td>2607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>525005120</td>\n",
       "      <td>2552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       names  degree\n",
       "0   36167088    3235\n",
       "1   17037258    2674\n",
       "2  707475104    2642\n",
       "3   22812156    2607\n",
       "4  525005120    2552"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_sorted = pd.DataFrame(sorted(G_tmp.degree, key=lambda x: x[1], reverse=True))\n",
    "G_sorted.columns = ['names','degree']\n",
    "dc = G_sorted\n",
    "G_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readability\n",
    "For easier readability of the visualization, we need to obtain the Twitter Handles of the users. For the sake of clarity we only obtain the first **100** user handles, which coincides with the **100** most influential nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list = list(G_sorted[\"names\"])\n",
    "sFollowingHandles_df = pd.DataFrame(columns=['Id','handle'])\n",
    "sFollowingHandles = []\n",
    "sFollowingHandles_list = []\n",
    "\n",
    "for i in range(100):\n",
    "    response = api.get_user(id=user_list[i])\n",
    "    sFollowingHandles.append(\"@\" + response.data.username)\n",
    "    print(str(user_list[i]) + ' ' + sFollowingHandles[i])\n",
    "    sFollowingHandles_list.append(sFollowingHandles)\n",
    "\n",
    "sFollowingHandles_df['handle'] = sFollowingHandles_list[0]\n",
    "sFollowingHandles_df['Id'] = user_list[:100]\n",
    "sFollowingHandles_df.to_csv(\"sFollowing/sFollowingHandles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export\n",
    "After the preliminary analysis and data cleaning, we know need to export the cleaned data for further analysis using Gephi. To do that we combine the sorted DataFrame with the Groups from the community detection algorithm, giving us a DataFrame of nodes. Then we convert the `G_tmp` NetworkX graph back into a Pandas Edgelist DataFrame. Then we export both DataFrames as `.csv` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.merge(dc, partition1, how='left', left_on='names', right_on='names')\n",
    "combined = combined.rename(columns={'names':'id'})\n",
    "\n",
    "edges = nx.to_pandas_edgelist(G_tmp)\n",
    "\n",
    "edges.to_csv(\"sFollowing/edges.csv\")\n",
    "combined.to_csv(\"sFollowing/nodes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"finalVis/sFollowing0.png\" alt=\"data-viz\" title=\"Viz\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific Groups\n",
    "\n",
    "From the vizualisation of the _secondary followers_ we find that there are two groups who stand out: The green and pink community. A cursory overview of the most influential nodes reveals a theme surrounding police and the recently cancelled TV Series _*Live PD*_. \n",
    "\n",
    "> So who are these accounts following?<br>\n",
    "> What is their sentiment?<br>\n",
    "> Are there any notable accounts that should be on watch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cop_df = pd.read_csv(\"copTwitter/copTwitterNodes.csv\")\n",
    "cop_df = cop_df.drop(columns=['Label','timeset','degree','group','componentnumber','strongcompnum','indegree','outdegree','Eccentricity','closnesscentrality','harmonicclosnesscentrality','betweenesscentrality'])\n",
    "\n",
    "sFollowers_df = pd.read_csv(\"sFollowers/sFollowers.csv\")\n",
    "sFollowers_df = sFollowers_df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list = list(cop_df['Id'])\n",
    "tertiary_df = pd.DataFrame(columns=['source','target'])\n",
    "\n",
    "for userID in user_list:\n",
    "    print(userID)\n",
    "    temp = sFollowers_df[sFollowers_df['source'] == userID]\n",
    "    tertiary_df = pd.concat([tertiary_df,temp],ignore_index=True)\n",
    "    tertiary_df.to_csv(\"copTwitter/tcFollowers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(tertiary_df,'source','target')\n",
    "\n",
    "partition = community_louvain.best_partition(G)\n",
    "\n",
    "partition1 = pd.DataFrame([partition]).T\n",
    "partition1 = partition1.reset_index()\n",
    "partition1.columns = ['names','group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_sorted = pd.DataFrame(sorted(G.degree, key=lambda x: x[1], reverse=True))\n",
    "G_sorted.columns = ['names', 'degree']\n",
    "dc = G_sorted\n",
    "G_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list = list(G_sorted['names'])\n",
    "tFollowerHandles_df = pd.DataFrame(columns=['names','Label'])\n",
    "tFollowerHandles = []\n",
    "tFollowerHandles_list = []\n",
    "\n",
    "for i in range(100):\n",
    "    response = api.get_user(id=user_list[i])\n",
    "    tFollowerHandles.append(\"@\" + response.data.username)\n",
    "    print(str(user_list[i]) + ' ' + tFollowerHandles[i])\n",
    "    tFollowerHandles_list.append(tFollowerHandles)\n",
    "\n",
    "tFollowerHandles_df['Label'] = tFollowerHandles_list[0]\n",
    "tFollowerHandles_df['names'] = user_list[:100]\n",
    "tFollowerHandles_df.to_csv(\"copTwitter/userHandles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.merge(dc, partition1, how='left', left_on='names', right_on='names')\n",
    "combined = pd.merge(combined, tFollowerHandles_df, how='left', left_on='names', right_on='names')\n",
    "combined = combined.rename(columns={'names': 'Id','Label_y': 'Label'})\n",
    "\n",
    "edges = nx.to_pandas_edgelist(G)\n",
    "\n",
    "edges.to_csv(\"copTwitter/edges.csv\")\n",
    "combined.to_csv(\"copTwitter/nodes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Results\n",
    "#### Notable Accounts\n",
    "**Group 0**<br>\n",
    "__[`@sgilks2166`](https://twitter.com/sgilks2166)__<br>\n",
    "__[`@CelataKaren`](https://twitter.com/CelataKaren)__<br>\n",
    "__[`@PermdogsSandy`](https://twitter.com/PermdogsSandy)__<br>\n",
    "__[`@MommyFayeee`](https://twitter.com/MommyFayeee)__<br>\n",
    "__[`@HallfordJeannie`](https://twitter.com/HallfordJeannie)__<br>\n",
    "__[`@TannamiaHall`](https://twitter.com/TannamiaHall)__<br>\n",
    "__[`@Jeffok16`](https://twitter.com/Jeffok16)__<br>\n",
    "__[`@Michael_KE7MT`](https://twitter.com/Michael_KE7MT)__<br>\n",
    "__[`@Mickey19741`](https://twitter.com/Mickey19741)__<br>\n",
    "**Group 28**<br>\n",
    "__[`@BIGRED476`](https://twitter.com/BIGRED476)__<br>\n",
    "__[`@Matt33822937`](https://twitter.com/Matt33822937)__<br>\n",
    "**Group 8**<br>\n",
    "__[`@resa2330`](https://twitter.com/resa2330)__<br>\n",
    "__[`@rebelbrat71`](https://twitter.com/rebelbrat71)__<br>\n",
    "**Group 10**<br>\n",
    "__[`@OedekovenTerry`](https://twitter.com/OedekovenTerry)__<br>\n",
    "**Group 23**<br>\n",
    "__[`@johnsons_nc`](https://twitter.com/johnsons_nc)__<br>\n",
    "**Group 15**<br>\n",
    "__[`@TXCoffeeSlinger`](https://twitter.com/TXCoffeeSlinger)__<br>\n",
    "**Group 21**<br>\n",
    "__[`@tambarry`](https://twitter.com/tambarry)__<br>\n",
    "\n",
    "<img src=\"finalVis/tcFollowers0.png\" alt-text=\"cop-data-viz\" title=\"Cop Twitter Data Visualization\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notable_accounts = ['sgilks2166','CelataKaren','PermdogsSandy','MommyFayeee','HallfordJeannie','TannamiaHall','Jeffok16','Michael_KE7MT','Mickey19741','BIGRED476','Matt33822937','resa2330','rebelbrat71','OedekovenTerry','johnsons_nc','TXCoffeeSlinger','tambarry']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('sna')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9031bf188e25d3bc949e5c8a042f665a15ac04e169b8b6b5e0691fc0c6e29585"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
